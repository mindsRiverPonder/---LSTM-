{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_comments(path):\n",
    "    '''读取评论和对应的评分'''\n",
    "    #这是我心血来潮写的数据预处理脑抽代码\n",
    "    with open(path,\"r\",encoding='utf-8') as fp:\n",
    "        #第一行是数据指标标签，不要，最后一行是一个空格，不要\n",
    "        reviews = fp.read().split('\\n')[1:-1]\n",
    "    #csv文件的每个格子以逗号划分\n",
    "    all_split_dot = [[i.start() for i in re.finditer(',',review)] for review in reviews]\n",
    "    #提取评分\n",
    "    labels = torch.tensor([int(float(reviews[i][all_split_dot[i][-1]+1:])) for i in range(len(all_split_dot))]) -1\n",
    "    #提取评论\n",
    "    comments = [reviews[i][all_split_dot[i][3]+1:all_split_dot[i][-1]].replace('\"','').replace(' ',\"\") for i in range(len(all_split_dot))]\n",
    "    #把每条评论进行分词分词\n",
    "    comments = [[i for i in jieba.cut(comment,cut_all=False)]for comment in comments]\n",
    "    return comments,labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_data(root,batch_size,num_steps=60):\n",
    "    \"\"\"返回数据迭代器和评论数据集的词表\"\"\"\n",
    "    #把评论和对应的评分取出来\n",
    "    comments,labels = read_comments(root)\n",
    "    #创建vocab，使得每个词都有对应的索引，将词频小于3的词不要了，不然这个vocab太大了\n",
    "    vocab = d2l.Vocab(comments, min_freq=3,reserved_tokens=['<pad>'])\n",
    "    #划分训练集和测试集\n",
    "    train_comment, test_comment, train_label, test_label = train_test_split(comments,labels,test_size=0.2)\n",
    "    #鉴于绝大部分评论都在60个词以下，把每一条评论都处理成相同长度（60个词），对短评论进行填充'pad',对长评论进行截断\n",
    "    train_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[comment], num_steps, vocab['<pad>']) for comment in train_comment])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[comment], num_steps, vocab['<pad>']) for comment in test_comment])\n",
    "    #返回数据迭代器\n",
    "    train_iter = d2l.load_array((train_features,train_label),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features,test_label),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将bidirectional设置为True以获取双向循环神经网络\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        #这里评分有5个等级\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是（批量大小，时间步数）\n",
    "        # 因为长短期记忆网络要求其输入的第一个维度是时间维，\n",
    "        # 所以在获得词元表示之前，输入会被转置。\n",
    "        # 输出形状为（时间步数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # 返回上一个隐藏层在不同时间步的隐状态，\n",
    "        # outputs的形状是（时间步数，批量大小，2*隐藏单元数）\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # 连结初始和最终时间步的隐状态，作为全连接层的输入，\n",
    "        # 其形状为（批量大小，4*隐藏单元数）\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            file_path)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, file_path):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "\n",
    "        #！！！！！这里得用bz2.open打开文件\n",
    "        with bz2.open(file_path, 'rb') as fp:\n",
    "            word_vecs = fp.readlines()\n",
    "        word_vecs = [i.decode('utf-8') for i in word_vecs][1:] #第一行信息没啥用，所以不要了\n",
    "        for vec in word_vecs:\n",
    "            #消除每一行后面的‘\\n’,已经后面多余空格，然后依据空格划分元素，形成一个列表\n",
    "            elems = vec.rstrip().rstrip('\\n').split(' ')\n",
    "            #每一行的第一个元素是词，剩余的元素是词向量\n",
    "            token,elems = elems[0],[float(elem) for elem in elems[1:]]\n",
    "            idx_to_token.append(token)\n",
    "            idx_to_vec.append(elems)\n",
    "\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, d2l.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[d2l.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(net, X, y, loss, trainer, device):\n",
    "\n",
    "    if isinstance(X, list):\n",
    "        X = [x.to(device) for x in X]\n",
    "    else:\n",
    "        X = X.to(device)\n",
    "\n",
    "    y = y.to(device)\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               device=d2l.try_gpu()):\n",
    "\n",
    "    #权重初始化\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.LSTM:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    #将预训练好的词嵌入加载到net的embeding层里，并且不进行梯度回传\n",
    "    net.embedding.weight.data.copy_(embeds)\n",
    "    net.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    num_batches = len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        #定义一个容器，里面存放着训练损失，训练准确度，样本数量\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "\n",
    "\n",
    "            l, acc = train_one_epoch(net, features, labels, loss, trainer, device)\n",
    "\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
    "                              None))\n",
    "\n",
    "        #查看测试集的准确率\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "\n",
    "    print(f'平均损失: {metric[0] / metric[2]:.3f}, 训练准确度: '\n",
    "          f'{metric[1] / metric[2]:.3f}, 测试准确度: {test_acc:.3f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 512                                   #批量大小\n",
    "lr = 0.0003                                        #学习率\n",
    "num_epochs = 20                                    #训练几轮\n",
    "embed_size = 300                                   #词嵌入维度，我选了300维的\n",
    "num_hiddens = 256                                  #循环神经网络，隐藏层单元数量\n",
    "num_layers = 2                                     #多少个隐藏层\n",
    "devices = d2l.try_gpu()                            #设备\n",
    "\n",
    "comment_path = ''     #数据集文件路径\n",
    "train_iter, test_iter, vocab = process_data(comment_path,batch_size)\n",
    "\n",
    "embed_path = ''     #词向量文件路径\n",
    "my_embedding = TokenEmbedding(embed_path)\n",
    "embeds = my_embedding[vocab.idx_to_token]          #把词向量和我的vocab结合起来\n",
    "\n",
    "\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)  #定义网络\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)           #优化器\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")                  #损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(net, train_iter, test_iter, loss, trainer,num_epochs,d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(net, vocab, comment):\n",
    "    net.eval()\n",
    "    comment = torch.tensor(vocab[[i for i in jieba.cut(comment,cut_all=False)]], device=d2l.try_gpu())\n",
    "    label = torch.argmax(net(comment.reshape(1, -1)), dim=1)\n",
    "    return label+1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(net,vocab,'包装很好,内容也不错')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
