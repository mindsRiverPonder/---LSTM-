{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read_comments(comment_path,mode=False):\n",
    "    with open(comment_path,'r',encoding='utf-8') as fp:\n",
    "        #第一行是数据指标标签，不要，最后一行是一个空格，不要\n",
    "        raw_data = fp.read().split('\\n')[1:-1]\n",
    "        #每一行数据，第一个字符是标签，后续字符是评论内容\n",
    "        comments = [raw_data[i][2:] for i in range(len(raw_data))]\n",
    "        lables = torch.tensor([int(raw_data[i][0]) for i in range(len(raw_data))])\n",
    "        #对评论进行分词,cut_all为分词模式\n",
    "        comments = [[i for i in jieba.cut(comment,cut_all=mode)]for comment in comments]\n",
    "        #返回分词后的评论和对应的评分\n",
    "    return comments,lables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def process_data(comment_path,batch_size,max_len,min_freq):\n",
    "    \"\"\"返回数据迭代器和评论数据集的词表\"\"\"\n",
    "    #把评论和对应的评分取出来\n",
    "    comments,labels = read_comments(comment_path)\n",
    "    #创建vocab，使得每个词都有对应的索引，将词频小于min_freq的词不要了，不然这个vocab太大了\n",
    "    vocab = d2l.Vocab(comments, min_freq=min_freq)\n",
    "    #划分训练集和测试集\n",
    "    train_comment, test_comment, train_label, test_label = train_test_split(comments,labels,test_size=0.3,random_state=26,shuffle=True)\n",
    "    #鉴于绝大部分评论都在max_len个词以下，把每一条评论都处理成相同长度（max_len个词），对短评论进行填充'<unk>',对长评论进行截断\n",
    "    train_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[comment], max_len, vocab['<unk>']) for comment in train_comment])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[comment], max_len, vocab['<unk>']) for comment in test_comment])\n",
    "    #返回数据迭代器\n",
    "    train_iter = d2l.load_array((train_features,train_label),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features,test_label),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "2000001"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TokenEmbedding:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            file_path)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, file_path):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "\n",
    "        with open(file_path, 'r') as fp:\n",
    "            word_vecs = fp.readlines()\n",
    "        word_vecs = [i for i in word_vecs][1:] #第一行信息没啥用，所以不要了\n",
    "        for vec in word_vecs:\n",
    "            #消除每一行后面的‘\\n’,已经后面多余空格，然后依据空格划分元素，形成一个列表\n",
    "            elems = vec.rstrip().rstrip('\\n').split(' ')\n",
    "            #每一行的第一个元素是词，剩余的元素是词向量\n",
    "            token,elems = elems[0],[float(elem) for elem in elems[1:]]\n",
    "            idx_to_token.append(token)\n",
    "            idx_to_vec.append(elems)\n",
    "\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, d2l.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[d2l.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer架构里面的Add & Norm layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, eps=1e-5):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape, eps=eps)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    FFN\n",
    "    \"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ffn_hidden, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(embed_dim, dropout)\n",
    "        self.ffn = PositionWiseFFN(embed_dim, ffn_hidden, embed_dim)\n",
    "        self.addnorm2 = AddNorm(embed_dim, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, need_weights=False)[0])\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class semi_bert(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, ffn_hiddens, num_heads,num_blks, dropout, max_len=80, **kwargs):\n",
    "        super(semi_bert, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                          embed_size)*0.01)\n",
    "        self.blks = nn.Sequential()\n",
    "\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", TransformerBlock(\n",
    "                embed_dim=embed_size, num_heads=num_heads, ffn_hidden=ffn_hiddens, dropout=dropout))\n",
    "        self.output = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "\n",
    "        # X的shape：(batch size, max_length,num_hiddens)\n",
    "        X = self.token_embedding(tokens) + self.pos_embedding\n",
    "        for blk in self.blks:\n",
    "            X = blk(X)\n",
    "        #获取句子的平均表示，而不是提取第一个字符\n",
    "        X = self.output(torch.mean(X, dim=1))\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(net, X, y, loss, trainer, device):\n",
    "\n",
    "    if isinstance(X, list):\n",
    "        X = [x.to(device) for x in X]\n",
    "    else:\n",
    "        X = X.to(device)\n",
    "\n",
    "    y = y.to(device)\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "\n",
    "    l = loss(pred, y)\n",
    "    l.mean().backward()\n",
    "    trainer.step()\n",
    "\n",
    "    train_loss_sum = l.sum()\n",
    "\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, trainer, num_epochs,device=d2l.try_gpu()):\n",
    "\n",
    "    #权重初始化\n",
    "    def initialize_weights(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith('token_embedding'):\n",
    "                continue  # 跳过词嵌入层\n",
    "\n",
    "            # 处理 TransformerBlock 中的层\n",
    "            if isinstance(param, nn.Linear):\n",
    "                if 'attention' in name:\n",
    "                    # 多头注意力层的线性层\n",
    "                    nn.init.xavier_uniform_(param.weight)\n",
    "                else:\n",
    "                    # FFN 中的线性层\n",
    "                    nn.init.kaiming_uniform_(param.weight, a=math.sqrt(5))\n",
    "                if param.bias is not None:\n",
    "                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(param.weight)\n",
    "                    bound = 1 / math.sqrt(fan_in)\n",
    "                    nn.init.uniform_(param.bias, -bound, bound)\n",
    "            elif isinstance(param, nn.LayerNorm):\n",
    "                nn.init.ones_(param.weight)\n",
    "                nn.init.zeros_(param.bias)\n",
    "\n",
    "    net.apply(initialize_weights)\n",
    "\n",
    "    #将预训练好的词嵌入加载到net的embeding层里，并且不进行梯度回传\n",
    "    #当然，你也可以进行训练\n",
    "    net.token_embedding.weight.data.copy_(embeds)\n",
    "    net.token_embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    num_batches = len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        #定义一个容器，里面存放着训练损失，训练准确度，样本数量\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "\n",
    "\n",
    "            l, acc = train_one_epoch(net, features, labels, loss, trainer, device)\n",
    "\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
    "                              None))\n",
    "\n",
    "        #查看测试集的准确率\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "\n",
    "    print(f'平均损失: {metric[0] / metric[2]:.3f}, 训练准确度: '\n",
    "          f'{metric[1] / metric[2]:.3f}, 测试准确度: {test_acc:.3f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\86180\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.875 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512                                   #批量大小\n",
    "lr = 0.0003                                        #学习率\n",
    "num_epochs = 30                                    #训练几轮\n",
    "embed_size = 100                                   #词嵌入维度，我选了100维的\n",
    "ffn_hiddens = 64                                   #FFN，隐藏层单元数量\n",
    "num_heads = 4                                      #注意力头的个数\n",
    "num_blks = 1                                       #transformer_block的个数\n",
    "dropout = 0.5                                      #dropout率（用于正则化）\n",
    "max_len = 50                                       #每个句子的最大长度\n",
    "min_freq = 3                                       #最小词频阈值\n",
    "devices = d2l.try_gpu()                            #设备\n",
    "\n",
    "embedding_path = '腾讯词向量/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'  #词向量的位置\n",
    "comment_path = 'weibo_senti_100k.csv'              #数据集的位置\n",
    "\n",
    "train_iter, test_iter, vocab = process_data(comment_path,batch_size,max_len=max_len,min_freq = min_freq)\n",
    "\n",
    "my_embedding = TokenEmbedding(embedding_path)\n",
    "embeds = my_embedding[vocab.idx_to_token]          #把词向量和我的vocab结合起来\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['@',\n '张晓鹏',\n 'jonathan',\n ' ',\n '土耳其',\n '的',\n '事要',\n '认真对待',\n '[',\n '哈哈',\n ']',\n '，',\n '否则',\n '直接',\n '开除',\n '。',\n '@',\n '丁丁',\n '看',\n '世界',\n ' ',\n '很',\n '是',\n '细心',\n '，',\n '酒店',\n '都',\n '全部',\n 'OK',\n '啦',\n '。']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = semi_bert(len(vocab), embed_size, ffn_hiddens, num_heads,num_blks,dropout,max_len)  #定义网络\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=0.0003,weight_decay=1e-4)     #优化器，使用Adam\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")     #损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(net, train_iter, test_iter, loss, trainer,num_epochs,d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lable_map = ['负向评论','正向评论']\n",
    "def predict(net, vocab, comment):\n",
    "    '''\n",
    "    对单个句子进行情感分析\n",
    "    '''\n",
    "    net.to(d2l.try_gpu())\n",
    "    net.eval()\n",
    "    comment = vocab[[i for i in jieba.cut(comment,cut_all=False)]]\n",
    "    #将句子进行分词\n",
    "    comment_pt = torch.tensor(d2l.truncate_pad(comment, max_len, vocab['<unk>']), device=d2l.try_gpu())\n",
    "    label = torch.argmax(net(comment_pt.reshape(1, -1)), dim=1)\n",
    "    return '该句子的情感是：' + lable_map[label]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(net, vocab, '堵心的我连灌饼都吃不下。。。。。。。')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_doc_visualize(net,vocab,test_path,out = False,draw = False):\n",
    "    '''\n",
    "    test_path：你的csv文件路径，注意：每条评论需要在第一列\n",
    "    对一个文档的句子进行情感分析，并绘制词云\n",
    "    out参数指明需不需要将句子和对应的情感输出出来\n",
    "    draw参数指明需不需要绘制词云\n",
    "    '''\n",
    "    net.to(d2l.try_gpu())\n",
    "    net.eval()\n",
    "    with open(test_path,'r',encoding = 'gbk') as fp:\n",
    "        raw_data = fp.read().split('\\n')[:-1]\n",
    "\n",
    "        comments = [raw_data[i] for i in range(len(raw_data))]\n",
    "        #对评论进行分词,cut_all为分词模式\n",
    "        commentss = [vocab[[i for i in jieba.cut(j,cut_all=False)]] for j in comments]\n",
    "        comment_pt = [torch.tensor(d2l.truncate_pad(i, max_len, vocab['<unk>']), device=d2l.try_gpu()) for i in commentss]\n",
    "        #预测出来的情感\n",
    "        labels = [torch.argmax(net(i.reshape(1, -1)), dim=1) for i in comment_pt]\n",
    "\n",
    "    def generate_wordcloud(comments, sentiment):\n",
    "        text = \" \".join(comments)\n",
    "\n",
    "        #font_path是字体文件，需要自己下载\n",
    "        wordcloud = WordCloud(background_color=\"white\",font_path=\"字体家AI造字特隶.ttf\").generate(text)\n",
    "        plt.figure(figsize=(8, 8), facecolor=None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.title(f\"Word Cloud - {sentiment} Sentiment\")\n",
    "        plt.show()\n",
    "\n",
    "    if draw:\n",
    "         #正向评论\n",
    "        positive_comments = []\n",
    "        #负向评论\n",
    "        negative_comments = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == 1:\n",
    "                positive_comments.append(comments[i])\n",
    "            else:\n",
    "                negative_comments.append(comments[i])\n",
    "        generate_wordcloud(positive_comments, \"Positive\")\n",
    "        generate_wordcloud(negative_comments, \"Negative\")\n",
    "\n",
    "\n",
    "\n",
    "    if out:\n",
    "        for i in range(len(comments)):\n",
    "            print(comments[i])\n",
    "            print('\\n')\n",
    "            print('*情感*--->'+lable_map[labels[i]])\n",
    "            print('\\n')\n",
    "            print('--------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_path = '微博测试.csv'\n",
    "predict_doc_visualize(net,vocab,test_path,out = False,draw = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}